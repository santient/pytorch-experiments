{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: We detected that you are running inside a Ipython/Jupyter notebook environment but we cannot save your notebook source code. Please be sure to have installed comet_ml as a notebook server extension by running:\n",
      "jupyter comet_ml enable \n",
      "For more details, please refer to: https://www.comet.ml/docs/python-sdk/warnings-errors\n",
      "COMET INFO: old comet version (1.0.29) detected. current: 1.0.31 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET WARNING: Comet.ml support for Ipython Notebook is limited at the moment, automatic monitoring and stdout capturing is deactivated \n",
      "For more details, please refer to: https://www.comet.ml/docs/python-sdk/warnings-errors\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/santient/pytorch-avb/7cf132e5da3c4224982654749e11e7bf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"E3oWJUSFulpXpCUQfc5oGz0zY\", project_name=\"pytorch-avb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# representation_size = 2\n",
    "# input_size = 4\n",
    "# n_samples = 2000\n",
    "# batch_size = 500\n",
    "# gen_hidden_size = 200\n",
    "# enc_hidden_size = 200\n",
    "# disc_hidden_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples_per_batch = n_samples//input_size\n",
    "\n",
    "# y = np.array([i for i in range(input_size)  for _ in range(n_samples_per_batch)])\n",
    "\n",
    "# d = np.identity(input_size)\n",
    "# x = np.array([d[i] for i in y], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x[[10, 58 ,610, 790, 1123, 1258, 1506, 1988]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAE(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(VAE, self).__init__()\n",
    "#         self.gen_l1 = torch.nn.Linear(representation_size, gen_hidden_size)\n",
    "#         self.gen_l2 = torch.nn.Linear(gen_hidden_size, input_size)\n",
    "        \n",
    "#         self.enc_l1 = torch.nn.Linear(input_size+representation_size, \n",
    "#                                       enc_hidden_size)\n",
    "#         self.enc_l2 = torch.nn.Linear(enc_hidden_size, representation_size)\n",
    "        \n",
    "#         self.disc_l1 = torch.nn.Linear(input_size+representation_size, \n",
    "#                                        disc_hidden_size)\n",
    "#         self.disc_l2 = torch.nn.Linear(disc_hidden_size, 1)\n",
    "        \n",
    "#         self.relu = torch.nn.ReLU()\n",
    "#         self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "#     def sample_prior(self, s):\n",
    "#         if self.training:\n",
    "#             m = torch.zeros((s.data.shape[0], representation_size))\n",
    "#             std = torch.ones((s.data.shape[0], representation_size))\n",
    "#             d = Variable(torch.normal(m,std)).cuda()\n",
    "#         else:\n",
    "#             d = Variable(torch.zeros((s.data.shape[0], representation_size))).cuda()\n",
    "        \n",
    "#         return d\n",
    "    \n",
    "#     def discriminator(self, x,z):\n",
    "#         i = torch.cat((x, z), dim=1).cuda()\n",
    "#         h = self.relu(self.disc_l1(i))\n",
    "#         return self.disc_l2(h)\n",
    "    \n",
    "#     def sample_posterior(self, x):\n",
    "#         i = torch.cat((x, self.sample_prior(x)), dim=1).cuda()\n",
    "#         h = self.relu(self.enc_l1(i))\n",
    "#         return self.enc_l2(h)\n",
    "    \n",
    "#     def decoder(self, z):\n",
    "#         i = self.relu(self.gen_l1(z))\n",
    "#         h = self.sigmoid(self.gen_l2(i))\n",
    "#         return h\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         z_p = self.sample_prior(x)\n",
    "        \n",
    "#         z_q = self.sample_posterior(x)\n",
    "#         log_d_prior = self.discriminator(x, z_p)\n",
    "#         log_d_posterior = self.discriminator(x, z_q)\n",
    "#         disc_loss = torch.mean(\n",
    "#             torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "#             log_d_posterior, torch.ones_like(log_d_posterior)\n",
    "#         )\n",
    "#         + torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "#             log_d_prior, torch.zeros_like(log_d_prior))\n",
    "#         )\n",
    "        \n",
    "#         x_recon = self.decoder(z_q)\n",
    "#         recon_liklihood = -torch.nn.functional.binary_cross_entropy(\n",
    "#                                                 x_recon, x)*x.data.shape[0]\n",
    "        \n",
    "#         gen_loss = torch.mean(log_d_posterior)-torch.mean(recon_liklihood)\n",
    "        \n",
    "#         return disc_loss, gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "channels = 3\n",
    "latent_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AVB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AVB, self).__init__()\n",
    "        \n",
    "        # height and width of downsampled image\n",
    "        self.ds_size = img_size // 2**4\n",
    "        \n",
    "        self.gen_proj = nn.Linear(latent_dim, 256*self.ds_size**2)\n",
    "        self.gen_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(256, 256, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(128, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(64, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.BatchNorm2d(32, 0.8),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(32, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose2d(32, channels, 3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.enc_proj = nn.Linear(latent_dim, img_size**2)\n",
    "        self.enc_blocks = nn.Sequential(\n",
    "            nn.Conv2d(channels+1, 32, 3, 1, 1),\n",
    "            nn.Conv2d(32, 32, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.BatchNorm2d(32, 0.8),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.Conv2d(64, 64, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.Conv2d(128, 128, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.Conv2d(256, 256, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.BatchNorm2d(256, 0.8)\n",
    "        )\n",
    "        self.enc_layer = nn.Linear(256*self.ds_size**2, latent_dim)\n",
    "        \n",
    "        self.dis_proj = nn.Linear(latent_dim, img_size**2)\n",
    "        self.dis_blocks = nn.Sequential(\n",
    "            nn.Conv2d(channels+1, 32, 3, 1, 1),\n",
    "            nn.Conv2d(32, 32, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.BatchNorm2d(32, 0.8),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.Conv2d(64, 64, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.Conv2d(128, 128, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.Conv2d(256, 256, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.BatchNorm2d(256, 0.8)\n",
    "        )\n",
    "        self.dis_layer = nn.Sequential(\n",
    "            nn.Linear(256*self.ds_size**2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def sample_prior(self, s):\n",
    "        if self.training:\n",
    "            m = torch.zeros((s.data.shape[0], latent_dim))\n",
    "            std = torch.ones((s.data.shape[0], latent_dim))\n",
    "            d = Variable(torch.normal(m,std))\n",
    "        else:\n",
    "            d = Variable(torch.zeros((s.data.shape[0], latent_dim)))\n",
    "        return d.cuda()\n",
    "    \n",
    "    def discriminator(self, x,z):\n",
    "        z_proj = self.dis_proj(z)\n",
    "        z_proj = z_proj.view(z_proj.shape[0], 1, img_size, img_size)\n",
    "        i = torch.cat((x, z_proj), dim=1)\n",
    "        h = self.dis_blocks(i)\n",
    "        h = h.view(h.shape[0], 256*self.ds_size**2)\n",
    "        out = self.dis_layer(h)\n",
    "        return out\n",
    "    \n",
    "    def sample_posterior(self, x):\n",
    "        prior_proj = self.enc_proj(self.sample_prior(x))\n",
    "        prior_proj = prior_proj.view(prior_proj.shape[0], 1, img_size, img_size)\n",
    "        i = torch.cat((x, prior_proj), dim=1)\n",
    "        h = self.enc_blocks(i)\n",
    "        h = h.view(h.shape[0], 256*self.ds_size**2)\n",
    "        out = self.enc_layer(h)\n",
    "        return out\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        z_proj = self.gen_proj(z)\n",
    "        z_proj = z_proj.view(z_proj.shape[0], 256, self.ds_size, self.ds_size)\n",
    "        out = self.gen_blocks(z_proj)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_p = self.sample_prior(x)\n",
    "        \n",
    "        z_q = self.sample_posterior(x)\n",
    "        log_d_prior = self.discriminator(x, z_p)\n",
    "        log_d_posterior = self.discriminator(x, z_q)\n",
    "        dis_loss = torch.mean(\n",
    "            torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            log_d_posterior, torch.ones_like(log_d_posterior)\n",
    "        )\n",
    "        + torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            log_d_prior, torch.zeros_like(log_d_prior))\n",
    "        )\n",
    "        \n",
    "        x_recon = self.decoder(z_q)\n",
    "        recon_likelihood = -torch.nn.functional.binary_cross_entropy(\n",
    "                                                x_recon, x)*x.data.shape[0]\n",
    "        \n",
    "        gen_loss = torch.mean(log_d_posterior)-torch.mean(recon_likelihood)\n",
    "        \n",
    "        return dis_loss, gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AVB().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVB(\n",
      "  (gen_proj): Linear(in_features=64, out_features=4096, bias=True)\n",
      "  (gen_blocks): Sequential(\n",
      "    (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (6): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (10): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (14): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): Sigmoid()\n",
      "  )\n",
      "  (enc_proj): Linear(in_features=64, out_features=4096, bias=True)\n",
      "  (enc_blocks): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (3): Dropout2d(p=0.25)\n",
      "    (4): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Dropout2d(p=0.25)\n",
      "    (9): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (12): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (13): Dropout2d(p=0.25)\n",
      "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (17): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (18): Dropout2d(p=0.25)\n",
      "    (19): BatchNorm2d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (enc_layer): Linear(in_features=4096, out_features=64, bias=True)\n",
      "  (dis_proj): Linear(in_features=64, out_features=4096, bias=True)\n",
      "  (dis_blocks): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (3): Dropout2d(p=0.25)\n",
      "    (4): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Dropout2d(p=0.25)\n",
      "    (9): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (12): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (13): Dropout2d(p=0.25)\n",
      "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (17): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (18): Dropout2d(p=0.25)\n",
      "    (19): BatchNorm2d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (dis_layer): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_params = []\n",
    "gen_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'dis' in name:\n",
    "        dis_params.append(param)\n",
    "    else:\n",
    "        gen_params.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_optimizer = torch.optim.Adam(dis_params, lr=1e-3)\n",
    "gen_optimizer = torch.optim.Adam(gen_params, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"/home/santiago/Downloads/celebA/\"\n",
    "\n",
    "batch_size = 128\n",
    "workers = 4\n",
    "dataset = datasets.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.CenterCrop(128),\n",
    "                               transforms.Resize(img_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=int(workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(batch_size, latent_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 1\n",
    "sample_interval = 500\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/202599]\td_loss: 0.011315\tg_loss: 0.716230\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done!0 [202496/202599]\td_loss: 0.009771\tg_loss: -13.462034\n",
      "Epoch 1 done!1 [202496/202599]\td_loss: 0.009771\tg_loss: -14.289164\n",
      "Epoch 2 done!2 [202496/202599]\td_loss: 0.009771\tg_loss: -13.746735\n",
      "Epoch 3 done!3 [202496/202599]\td_loss: 0.009771\tg_loss: -14.212850\n",
      "Epoch 4 done!4 [202496/202599]\td_loss: 0.009771\tg_loss: -13.454019\n",
      "Epoch 5 done!5 [202496/202599]\td_loss: 0.009771\tg_loss: -13.627545\n",
      "Epoch 6 done!6 [202496/202599]\td_loss: 0.009771\tg_loss: -13.152081\n",
      "Epoch 7 done!7 [202496/202599]\td_loss: 0.009771\tg_loss: -13.807567\n",
      "Epoch 8 done!8 [202496/202599]\td_loss: 0.009771\tg_loss: -13.210717\n",
      "Epoch 9 done!9 [202496/202599]\td_loss: 0.009771\tg_loss: -12.999068\n",
      "Epoch 10 done!0 [202496/202599]\td_loss: 0.009771\tg_loss: -13.798413\n",
      "Epoch 11 done!1 [202496/202599]\td_loss: 0.009771\tg_loss: -13.287612\n",
      "Epoch 12 done!2 [202496/202599]\td_loss: 0.009771\tg_loss: -13.218492\n",
      "Epoch 13 done!3 [202496/202599]\td_loss: 0.009771\tg_loss: -12.596121\n",
      "Epoch 14 done!4 [202496/202599]\td_loss: 0.009771\tg_loss: -13.563672\n",
      "Epoch 15 done!5 [202496/202599]\td_loss: 0.009771\tg_loss: -12.738002\n",
      "Epoch 16 done!6 [202496/202599]\td_loss: 0.009771\tg_loss: -12.820297\n",
      "Epoch 17 done!7 [202496/202599]\td_loss: 0.009771\tg_loss: -12.513060\n",
      "Epoch 18 done!8 [202496/202599]\td_loss: 0.009771\tg_loss: -13.286076\n",
      "Epoch 19 done!9 [202496/202599]\td_loss: 0.009771\tg_loss: -13.468997\n",
      "Epoch 20 done!0 [202496/202599]\td_loss: 0.009771\tg_loss: -13.093602\n",
      "Epoch 21 done!1 [202496/202599]\td_loss: 0.009771\tg_loss: -14.532279\n",
      "Epoch 22 done!2 [202496/202599]\td_loss: 0.009771\tg_loss: -13.272237\n",
      "Epoch 23 done!3 [202496/202599]\td_loss: 0.009771\tg_loss: -13.079411\n",
      "Epoch 24 done!4 [202496/202599]\td_loss: 0.009771\tg_loss: -13.965928\n",
      "Epoch 25 done!5 [202496/202599]\td_loss: 0.009771\tg_loss: -13.754634\n",
      "Epoch 26 done!6 [202496/202599]\td_loss: 0.009771\tg_loss: -13.459702\n",
      "Epoch 27 done!7 [202496/202599]\td_loss: 0.009771\tg_loss: -14.024043\n",
      "Epoch 28 done!8 [202496/202599]\td_loss: 0.009771\tg_loss: -13.152015\n",
      "Epoch 29 done!9 [202496/202599]\td_loss: 0.009771\tg_loss: -13.935898\n",
      "Epoch 30 done!0 [202496/202599]\td_loss: 0.009771\tg_loss: -14.338731\n",
      "Epoch 31 done!1 [202496/202599]\td_loss: 0.009771\tg_loss: -14.534057\n",
      "Epoch 32 done!2 [202496/202599]\td_loss: 0.009771\tg_loss: -12.900085\n",
      "Epoch 33 done!3 [202496/202599]\td_loss: 0.009771\tg_loss: -13.780727\n",
      "Epoch 34 done!4 [202496/202599]\td_loss: 0.009771\tg_loss: -12.558958\n",
      "Epoch 35 done!5 [202496/202599]\td_loss: 0.009771\tg_loss: -12.934898\n",
      "Epoch 36 done!6 [202496/202599]\td_loss: 0.009771\tg_loss: -13.466209\n",
      "Epoch 37 done!7 [202496/202599]\td_loss: 0.009771\tg_loss: -14.739004\n",
      "Epoch 38 done!8 [202496/202599]\td_loss: 0.009771\tg_loss: -13.689457\n",
      "Epoch 39 done!9 [202496/202599]\td_loss: 0.009771\tg_loss: -13.259332\n",
      "Epoch 40 done!0 [202496/202599]\td_loss: 0.009771\tg_loss: -12.615015\n",
      "Epoch 41 done!1 [202496/202599]\td_loss: 0.009771\tg_loss: -12.994614\n",
      "Epoch 42 done!2 [202496/202599]\td_loss: 0.009771\tg_loss: -14.380199\n",
      "Epoch 43 done!3 [202496/202599]\td_loss: 0.009771\tg_loss: -13.312529\n",
      "Epoch 44 done!4 [202496/202599]\td_loss: 0.009771\tg_loss: -13.641933\n",
      "Epoch 45 done!5 [202496/202599]\td_loss: 0.009771\tg_loss: -14.253270\n",
      "Epoch 46 done!6 [202496/202599]\td_loss: 0.009771\tg_loss: -13.432813\n",
      "Epoch 47 done!7 [202496/202599]\td_loss: 0.009771\tg_loss: -14.547152\n",
      "Epoch 48 done!8 [202496/202599]\td_loss: 0.009771\tg_loss: -14.514446\n",
      "Epoch 49 done!9 [202496/202599]\td_loss: 0.009771\tg_loss: -13.351841\n",
      "Epoch 50 done!0 [202496/202599]\td_loss: 0.009771\tg_loss: -13.929830\n",
      "Epoch 51 done!1 [202496/202599]\td_loss: 0.009771\tg_loss: -15.409242\n",
      "Epoch 52 done!2 [202496/202599]\td_loss: 0.009771\tg_loss: -13.737757\n",
      "Epoch 53 done!3 [202496/202599]\td_loss: 0.009771\tg_loss: -14.433719\n",
      "Epoch 54 done!4 [202496/202599]\td_loss: 0.009771\tg_loss: -14.734844\n",
      "Epoch 55 done!5 [202496/202599]\td_loss: 0.009771\tg_loss: -14.314102\n",
      "Epoch 56 done!6 [202496/202599]\td_loss: 0.009771\tg_loss: -13.802327\n",
      "Epoch 57 done!7 [202496/202599]\td_loss: 0.009771\tg_loss: -14.340333\n",
      "Epoch 58 done!8 [202496/202599]\td_loss: 0.009771\tg_loss: -14.618098\n",
      "Epoch 59 done!9 [202496/202599]\td_loss: 0.009771\tg_loss: -12.839678\n",
      "Epoch 60 done!0 [202496/202599]\td_loss: 0.009771\tg_loss: -12.899444\n",
      "Epoch 61 done!1 [202496/202599]\td_loss: 0.009771\tg_loss: -13.106295\n",
      "Epoch 62 done!2 [202496/202599]\td_loss: 0.009771\tg_loss: -14.421600\n",
      "Epoch 63 done!3 [202496/202599]\td_loss: 0.009771\tg_loss: -13.195252\n",
      "Epoch 64 done!4 [202496/202599]\td_loss: 0.009771\tg_loss: -14.817740\n",
      "Epoch 65 done!5 [202496/202599]\td_loss: 0.009771\tg_loss: -15.423512\n",
      "Epoch 66 done!6 [202496/202599]\td_loss: 0.009771\tg_loss: -14.542144\n",
      "Epoch 67 done!7 [202496/202599]\td_loss: 0.009771\tg_loss: -13.951866\n",
      "Epoch 68 done!8 [202496/202599]\td_loss: 0.009771\tg_loss: -14.031644\n",
      "Epoch 69 done!9 [202496/202599]\td_loss: 0.009771\tg_loss: -15.278087\n",
      "Epoch 70 done!0 [202496/202599]\td_loss: 0.009771\tg_loss: -14.836351\n",
      "Epoch 71 done!1 [202496/202599]\td_loss: 0.009771\tg_loss: -14.265303\n",
      "Epoch 72 done!2 [202496/202599]\td_loss: 0.009771\tg_loss: -14.823900\n",
      "Epoch 73 done!3 [202496/202599]\td_loss: 0.009771\tg_loss: -13.626368\n",
      "Epoch 74 done!4 [202496/202599]\td_loss: 0.009771\tg_loss: -14.828446\n",
      "Epoch 75 done!5 [202496/202599]\td_loss: 0.009771\tg_loss: -15.115224\n",
      "Epoch 76 done!6 [202496/202599]\td_loss: 0.009771\tg_loss: -15.177337\n",
      "Epoch 77 done!7 [202496/202599]\td_loss: 0.009771\tg_loss: -15.506713\n",
      "Epoch 78 done!8 [202496/202599]\td_loss: 0.009771\tg_loss: -13.986558\n",
      "Epoch 79 done!9 [202496/202599]\td_loss: 0.009771\tg_loss: -14.669578\n",
      "Epoch 80 done!0 [202496/202599]\td_loss: 0.009771\tg_loss: -13.047432\n",
      "Epoch 81 done!1 [202496/202599]\td_loss: 0.009771\tg_loss: -15.113247\n",
      "Epoch 82 done!2 [202496/202599]\td_loss: 0.009771\tg_loss: -13.791922\n",
      "Epoch 83 done!3 [202496/202599]\td_loss: 0.009771\tg_loss: -14.428191\n",
      "Epoch 84 done!4 [202496/202599]\td_loss: 0.009771\tg_loss: -14.337914\n",
      "Epoch 85 done!5 [202496/202599]\td_loss: 0.009771\tg_loss: -14.223490\n",
      "Epoch 86 done!6 [202496/202599]\td_loss: 0.009771\tg_loss: -13.198799\n",
      "Epoch 87 done!7 [202496/202599]\td_loss: 0.009771\tg_loss: -13.605265\n",
      "Epoch 88 done!8 [202496/202599]\td_loss: 0.009771\tg_loss: -13.476829\n",
      "Epoch 89 done!9 [202496/202599]\td_loss: 0.009771\tg_loss: -14.869698\n",
      "Epoch 90 done!0 [202496/202599]\td_loss: 0.009771\tg_loss: -13.942733\n",
      "Epoch 91 done!1 [202496/202599]\td_loss: 0.009771\tg_loss: -14.907041\n",
      "Epoch 92 done!2 [202496/202599]\td_loss: 0.009771\tg_loss: -14.501390\n",
      "Epoch 93 done!3 [202496/202599]\td_loss: 0.009771\tg_loss: -14.699249\n",
      "Epoch 94 done!4 [202496/202599]\td_loss: 0.009771\tg_loss: -15.174936\n",
      "Epoch 95 done!5 [202496/202599]\td_loss: 0.009771\tg_loss: -14.045699\n",
      "Epoch 96 done!6 [202496/202599]\td_loss: 0.009771\tg_loss: -13.501070\n",
      "Epoch 97 done!7 [202496/202599]\td_loss: 0.009771\tg_loss: -13.137969\n",
      "Epoch 98 done!8 [202496/202599]\td_loss: 0.009771\tg_loss: -14.814258\n",
      "Epoch 99 done!9 [202496/202599]\td_loss: 0.009771\tg_loss: -14.784561\n"
     ]
    }
   ],
   "source": [
    "with experiment.train():\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            data = Variable(data[0].cuda(), requires_grad=False)\n",
    "\n",
    "            dis_loss, gen_loss = model(data)\n",
    "\n",
    "            gen_optimizer.zero_grad()\n",
    "            gen_loss.backward(retain_graph=True)\n",
    "            gen_optimizer.step()\n",
    "\n",
    "            dis_optimizer.zero_grad()\n",
    "            dis_loss.backward(retain_graph=True)\n",
    "            dis_optimizer.step()\n",
    "\n",
    "            if (i % log_interval == 0) and (epoch % 1 == 0):\n",
    "                print('Train Epoch: {} [{}/{}]\\td_loss: {:.6f}\\tg_loss: {:.6f}'.format(\n",
    "                    epoch, i * batch_size, len(dataset),\n",
    "                    dis_loss.data[0] / len(data), gen_loss.data[0] / len(data)), end='\\r', flush=True)\n",
    "                experiment.log_metric(\"d_loss\", dis_loss.data[0] / len(data), step=batches_done)\n",
    "                experiment.log_metric(\"g_loss\", gen_loss.data[0] / len(data), step=batches_done)\n",
    "\n",
    "            if (i % sample_interval == 0) and (epoch % 1 == 0):\n",
    "                vutils.save_image(data,\n",
    "                            '../avb/images/real_samples.png',\n",
    "                            normalize=True)\n",
    "                fake = model.decoder(fixed_noise)\n",
    "                vutils.save_image(fake.detach(),\n",
    "                            '../avb/images/fake_samples_step_%03d.png' % batches_done,\n",
    "                            normalize=True)\n",
    "                # do checkpointing\n",
    "                torch.save(model.state_dict(), '../avb/checkpoints/avb_step_%d.pth' % batches_done)\n",
    "                torch.save(gen_optimizer.state_dict(), '../avb/checkpoints/gen_opt_step_%d.pth' % batches_done)\n",
    "                torch.save(dis_optimizer.state_dict(), '../avb/checkpoints/dis_opt_step_%d.pth' % batches_done)\n",
    "\n",
    "            batches_done += 1\n",
    "        print(\"Epoch {} done!\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vutils.save_image(data,\n",
    "            '../avb/images/real_samples.png',\n",
    "            normalize=True)\n",
    "fake = model.decoder(fixed_noise)\n",
    "vutils.save_image(fake.detach(),\n",
    "            '../avb/images/fake_samples_step_%03d.png' % batches_done,\n",
    "            normalize=True)\n",
    "# do checkpointing\n",
    "torch.save(model.state_dict(), '../avb/checkpoints/avb_step_%d.pth' % batches_done)\n",
    "torch.save(gen_optimizer.state_dict(), '../avb/checkpoints/gen_opt_step_%d.pth' % batches_done)\n",
    "torch.save(dis_optimizer.state_dict(), '../avb/checkpoints/dis_opt_step_%d.pth' % batches_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=1, sample_interval=100):\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        data = Variable(data[0].cuda(), requires_grad=False)\n",
    "        \n",
    "        dis_loss, gen_loss = model(data)\n",
    "        \n",
    "        gen_optimizer.zero_grad()\n",
    "        gen_loss.backward(retain_graph=True)\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "        dis_optimizer.zero_grad()\n",
    "        dis_loss.backward(retain_graph=True)\n",
    "        dis_optimizer.step()\n",
    "        \n",
    "        if (i % log_interval == 0) and (epoch % 1 == 0):\n",
    "            print('Train Epoch: {} [{}/{}]\\td_loss: {:.6f}\\tg_loss: {:.6f}'.format(\n",
    "                epoch, i * batch_size, len(dataset),\n",
    "                dis_loss.data[0] / len(data), gen_loss.data[0] / len(data)), flush=True)\n",
    "        \n",
    "        if (i % sample_interval == 0) and (epoch % 1 == 0):\n",
    "            vutils.save_image(data,\n",
    "                        '../avb/images/real_samples.png',\n",
    "                        normalize=True)\n",
    "            fake = model.decoder(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),\n",
    "                        '../avb/images/fake_samples_step_%03d.png' % batches_done,\n",
    "                        normalize=True)\n",
    "            # do checkpointing\n",
    "            torch.save(model.state_dict(), '../avb/checkpoints/avb_step_%d.pth' % batches_done)\n",
    "            torch.save(gen_optimizer.state_dict(), '../avb/checkpoints/gen_opt_step_%d.pth' % batches_done)\n",
    "            torch.save(dis_optimizer.state_dict(), '../avb/checkpoints/dis_opt_step_%d.pth' % batches_done)\n",
    "        \n",
    "        batches_done += 1\n",
    "#     ind = np.arange(x.shape[0])\n",
    "#     for i in range(batches_per_epoch):\n",
    "#         data = torch.from_numpy(x[np.random.choice(ind, size=batch_size)])\n",
    "#         data = Variable(data.cuda(), requires_grad=False)\n",
    "        \n",
    "        \n",
    "#         discrim_loss, gen_loss= model(data)\n",
    "        \n",
    "#         gen_optimizer.zero_grad()\n",
    "#         gen_loss.backward(retain_graph=True)\n",
    "#         gen_optimizer.step()\n",
    "        \n",
    "#         disc_optimizer.zero_grad()\n",
    "#         discrim_loss.backward(retain_graph=True)\n",
    "#         disc_optimizer.step()\n",
    "#         if (i % log_interval == 0) and (epoch % 1 ==0):\n",
    "#             #Print progress\n",
    "#             print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, i * batch_size, batch_size*batches_per_epoch,\n",
    "#                 discrim_loss.data[0] / len(data), gen_loss.data[0] / len(data)))\n",
    "\n",
    "#     print('====> Epoch: {} done!'.format(\n",
    "#           epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 15):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Variable(torch.from_numpy(x), requires_grad=False).cuda()\n",
    "\n",
    "model.train()\n",
    "zs = model.sample_posterior(data).data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter(zs[:,0], zs[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Variable(torch.from_numpy(x), requires_grad=False).cuda()\n",
    "model.eval()\n",
    "zs = model.sample_posterior(data).data.cpu().numpy()\n",
    "\n",
    "plt.scatter(zs[:,0], zs[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_point = np.array([0.5, 0.6], dtype=np.float32).reshape(1,-1)\n",
    "test_point = Variable(torch.from_numpy(test_point), requires_grad=False).cuda()\n",
    "s = model.decoder(test_point)\n",
    "s.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
